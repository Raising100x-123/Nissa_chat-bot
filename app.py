import sys
print("==== PYTHON VERSION ====")
print(sys.version)
print("========================")
import subprocess
from flask import Response

from flask import Flask, request, jsonify, render_template, session
import os
from dotenv import load_dotenv
import google.generativeai as genai
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains import create_retrieval_chain
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_community.vectorstores import MongoDBAtlasVectorSearch
from langchain_core.messages import HumanMessage
from langchain.chains import create_history_aware_retriever
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from pymongo import MongoClient
import secrets
import json
import time
import requests
from requests.auth import HTTPDigestAuth
from datetime import datetime
from pydub import AudioSegment
import uuid
from flask_cors import CORS
import glob
from pathlib import Path
import json
import base64                                   # <-- ADDED
from io import BytesIO                          # <-- ADDED
# ==== ElevenLabs Integration – imports start ====
from elevenlabs import ElevenLabs, VoiceSettings  # pip install elevenlabs
# ==== ElevenLabs Integration – imports end   ====
# Load environment variables
load_dotenv()

# Initialize Flask app
app = Flask(__name__)
CORS(app)
app.secret_key = os.getenv("FLASK_SECRET_KEY", secrets.token_urlsafe(16))

# Environment variables
MONGODB_URI = os.getenv("MONGODB_URI", "mongodb://localhost:27017/")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "")
DOCUMENTS_FOLDER = os.getenv("DOCUMENTS_FOLDER", "./data/")
GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-1.5-pro")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "models/embedding-001")
LLM_TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", "0.7"))

# ==== ElevenLabs Integration – env start ====

ELEVENLABS_API_KEY = os.getenv("ELEVENLABS_API_KEY", "")

ELEVENLABS_VOICE_ID= os.getenv("ELEVENLABS_VOICE_ID", "ZUrEGyu8GFMwnHbvLhv2")  # default English voice

# ==== ElevenLabs Integration – env end   ====

# MongoDB Atlas Search Index configuration
ATLAS_PUBLIC_KEY = os.getenv("ATLAS_PUBLIC_KEY")
ATLAS_PRIVATE_KEY = os.getenv("ATLAS_PRIVATE_KEY")
ATLAS_GROUP_ID = os.getenv("ATLAS_GROUP_ID")
ATLAS_CLUSTER_NAME = os.getenv("ATLAS_CLUSTER_NAME")
DATABASE_NAME = "Gemini"
INDEX_NAME = "vector_index"

# MongoDB setup
client = MongoClient(MONGODB_URI)
db = client.Gemini
chat_collection = db.chat_history
lead_collection = db.lead_data
collection_name = "customer_data"

# Configure Google AI
genai.configure(api_key=GOOGLE_API_KEY)

# Initialize Gemini models
llm = ChatGoogleGenerativeAI(
    model=GEMINI_MODEL,
    temperature=LLM_TEMPERATURE,
    google_api_key=GOOGLE_API_KEY
)

# Initialize embeddings
embeddings = GoogleGenerativeAIEmbeddings(
    model=EMBEDDING_MODEL,
    google_api_key=GOOGLE_API_KEY
)

# Lead extraction model with lower temperature for precision
lead_extraction_llm = ChatGoogleGenerativeAI(
    model=GEMINI_MODEL,
    temperature=0.1,
    google_api_key=GOOGLE_API_KEY
)



# ==== ElevenLabs Integration – client start ====

eleven_client = ElevenLabs(api_key=ELEVENLABS_API_KEY) if ELEVENLABS_API_KEY else None
# guard if key missing
if not eleven_client:
    print("[WARN] ELEVENLABS_API_KEY not set – voice endpoints will 503.")
# ==== ElevenLabs Integration – client end   ====
# ------------------------------------------------
#  3 · PROMPTS & HELPERS


# Prompt templates
CONTEXT_SYSTEM_PROMPT = """Given a chat history and the latest user question \
which might reference context in the chat history, formulate a standalone question \
which can be understood without the chat history. Do NOT answer the question, \
just reformulate it if needed and otherwise return it as is."""


QA_SYSTEM_PROMPT = """
Your name is Nisaa – the smart virtual assistant on this website. Follow these operating instructions:
 
I. 🎯 Purpose:
You assist visitors with clear, helpful answers based **only** on the provided context. Your responses should be concise, either in the form of a short summary or in **2–3 natural lines**. You also guide users toward sharing their details and booking expert consultations.
 
⚠️ If asked something outside the provided context, say:
"I can only provide details based on our official documents. For anything else, please contact our team directly."
 
II. 🗣️ Tone & Style:
- Warm, professional, emotionally intelligent
- Keep all responses natural and human-like
- Never sound robotic, overly technical, or salesy
- Replies must be **no longer than 2–3 lines**, unless a brief summary is needed
 
III. 💬 First Message:
On greeting, respond with:
"Hi, this is Nisaa! It’s nice to meet you here. How can I assist you today?"
 
IV. 🔄 Lead Capture Flow:
1. Begin by helping — **do not** ask for personal info in the first two replies.
2. After your second helpful response (around the 3rd message), ask:
   “By the way, may I know your name? It’s always nice to help you personally.”
3. If the user doesn’t provide a name, gently follow up:
   “Just before we move forward, may I please know your name? It helps me assist you better.”
4. Once the name is shared, continue naturally and use it in responses.
5. On the 5th–6th message, ask:
   - “Would you like me to email this to you?”
   - “Also, may I have your phone number in case our team needs to follow up?”
6. Ask for their **service interest**, and offer to schedule an expert consultation.
7. Keep it human — ask a **maximum of 2 questions per message**.
 
V. 💡 Hook Prompts (only after name is shared):
- “Would you like help choosing the right service?”
- “Want to see how others use this?”
- “Shall I walk you through a real example?”
- “Would you like to try a demo of this?”
- “Interested in seeing how this helped other clients?”
 
VI. 📞 Booking an Expert Call:
- Ask for topic/service of interest
- Ask for their preferred date and time
- Confirm schedule
- Collect name (if not already)
- Collect email and phone number
- Confirm the booking and offer a reminder
 
VII. 🔁 Fallback Handling:
- If repeated: “Let me explain that again, no worries.”
- If inactive: “Still there? I’m right here if you need anything.”
- If ending: “It’s been a pleasure! Come back anytime.”
 
VIII. 📝 Message Format:
- Keep all replies short (2–3 lines) or give a brief summary when needed
- Use bullet points for listing services
- Do not include external links
- Never use emojis unless explicitly requested
 
Context: {context}  
Chat History: {chat_history}  
Question: {input}  
 
Answer (based strictly on context, in short summary or 2–3 friendly lines. Only use CTA/hooks **after name is known**):
"""
 
 


LEAD_EXTRACTION_PROMPT = """
Extract the following information from the conversation if available:
- name
- email_id
- contact_number
- location
- service_interest
- appointment_date
- appointment_time

Return ONLY a valid JSON object with these fields with NO additional text before or after.
If information isn't found, leave the field empty.

Do not include any explanatory text, notes, or code blocks. Return ONLY the raw JSON.

Conversation: {conversation}
"""

# Create prompt templates
contextualize_q_prompt = ChatPromptTemplate.from_messages([
    ("system", CONTEXT_SYSTEM_PROMPT),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),
])

qa_prompt = ChatPromptTemplate.from_messages([
    ("system", QA_SYSTEM_PROMPT),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),
])

# Chat history management
chat_store = {}

def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in chat_store:
        chat_store[session_id] = ChatMessageHistory()
    return chat_store[session_id]

# ==== ElevenLabs Integration – helper functions start ====
# def tts_generate(text: str, voice_id: str = ELEVENLABS_VOICE_ID) -> bytes:
#     """Generate MP3 bytes from text using ElevenLabs."""
#     if not eleven_client:
#         raise RuntimeError("ElevenLabs client not configured")
#     try:
#         stream = eleven_client.text_to_speech.convert(
#             voice_id=voice_id,
#             text=text,
#             model_id="eleven_turbo_v2_5",
#             output_format="mp3_22050_32",
#             voice_settings=VoiceSettings()
#         )
#         audio_bytes = b"".join(stream)
#         print("🎤 TTS generated length:", len(audio_bytes))
#         return audio_bytes
#     except Exception as e:
#         import traceback
#         print("🎤 TTS failed:", e)
#         traceback.print_exc()
#         raise


def tts_generate(text: str, voice_id: str = ELEVENLABS_VOICE_ID) -> bytes:
    """Enhanced TTS with debugging"""
    print(f"🎵 TTS: Starting generation for {len(text)} characters")
    
    if not eleven_client:
        raise RuntimeError("ElevenLabs client not configured")
    
    if not text or not text.strip():
        raise ValueError("No text provided for TTS")
    
    # Limit text length
    if len(text) > 3000:
        text = text[:3000] + "..."
        print(f"🎵 TTS: Text truncated to 3000 characters")
    
    try:
        print(f"🎵 TTS: Calling ElevenLabs API with voice {voice_id}...")
        
        stream = eleven_client.text_to_speech.convert(
            voice_id=voice_id,
            text=text,
            model_id="eleven_turbo_v2_5",
            output_format="mp3_22050_32",
            voice_settings=VoiceSettings(
                stability=0.75,
                similarity_boost=0.8,
                style=0.0,
                use_speaker_boost=True
            )
        )
        
        print("🎵 TTS: Collecting audio stream...")
        audio_bytes = b"".join(stream)
        
        if not audio_bytes:
            raise RuntimeError("TTS generated empty audio")
        
        print(f"🎵 TTS: Generated {len(audio_bytes)} bytes")
        return audio_bytes
        
    except Exception as e:
        print(f"🎵 TTS: Error occurred: {str(e)}")
        raise

# def stt_transcribe(audio_bytes: bytes, language_code: str | None = None) -> str:
#     """Transcribe user speech with ElevenLabs Scribe v1 (99‑lang auto-detect)."""
#     if not eleven_client:
#         raise RuntimeError("ElevenLabs client not configured")
#     print("✅ Starting STT transcription...")
#     resp = eleven_client.speech_to_text.convert(
#         file=BytesIO(audio_bytes),
#         model_id="scribe_v1",
#         language_code=language_code,
#         diarize=False,       
#     )
#     return resp.text
def stt_transcribe(audio_bytes: bytes, language_code: str | None = None) -> str:
    """Enhanced STT with debugging"""
    print(f"🗣️  STT: Starting transcription for {len(audio_bytes)} bytes")
    
    if not eleven_client:
        raise RuntimeError("ElevenLabs client not configured")
    
    if not audio_bytes:
        raise ValueError("No audio data provided")
    
    try:
        print("🗣️  STT: Calling ElevenLabs API...")
        audio_io = BytesIO(audio_bytes)
        
        resp = eleven_client.speech_to_text.convert(
            file=audio_io,
            model_id="scribe_v1",
            language_code=language_code,
            diarize=False,
        )
        
        print(f"🗣️  STT: API response received")
        
        if not resp or not hasattr(resp, 'text'):
            raise RuntimeError("Invalid response from STT service")
        
        transcript = resp.text.strip() if resp.text else ""
        print(f"🗣️  STT: Final transcript: '{transcript}'")
        
        return transcript
        
    except Exception as e:
        print(f"🗣️  STT: Error occurred: {str(e)}")
        raise
# ==== ElevenLabs Integration – helper functions end   ====
# Atlas Search Index management functions
def create_atlas_search_index():
    url = f"https://cloud.mongodb.com/api/atlas/v2/groups/{ATLAS_GROUP_ID}/clusters/{ATLAS_CLUSTER_NAME}/search/indexes"
    headers = {'Content-Type': 'application/json', 'Accept': 'application/vnd.atlas.2024-05-30+json'}
    data = {
        "collectionName": collection_name,
        "database": DATABASE_NAME,
        "name": INDEX_NAME,
        "type": "vectorSearch",
        "definition": {
            "fields": [
                {"type": "vector", "path": "embedding", "numDimensions": 768, "similarity": "cosine"}
            ]
        }
    }
    response = requests.post(
        url, 
        headers=headers, 
        auth=HTTPDigestAuth(ATLAS_PUBLIC_KEY, ATLAS_PRIVATE_KEY), 
        data=json.dumps(data)
    )
    if response.status_code != 201:
        raise Exception(f"Failed to create Atlas Search Index: {response.status_code}, Response: {response.text}")
    return response

def get_atlas_search_index():
    url = f"https://cloud.mongodb.com/api/atlas/v2/groups/{ATLAS_GROUP_ID}/clusters/{ATLAS_CLUSTER_NAME}/search/indexes/{DATABASE_NAME}/{collection_name}/{INDEX_NAME}"
    headers = {'Accept': 'application/vnd.atlas.2024-05-30+json'}
    response = requests.get(
        url, 
        headers=headers, 
        auth=HTTPDigestAuth(ATLAS_PUBLIC_KEY, ATLAS_PRIVATE_KEY)
    )
    return response

def delete_atlas_search_index():
    url = f"https://cloud.mongodb.com/api/atlas/v2/groups/{ATLAS_GROUP_ID}/clusters/{ATLAS_CLUSTER_NAME}/search/indexes/{DATABASE_NAME}/{collection_name}/{INDEX_NAME}"
    headers = {'Accept': 'application/vnd.atlas.2024-05-30+json'}
    response = requests.delete(
        url, 
        headers=headers, 
        auth=HTTPDigestAuth(ATLAS_PUBLIC_KEY, ATLAS_PRIVATE_KEY)
    )
    return response

def load_multiple_documents():
    """Load multiple PDF and text files from the documents folder"""
    documents = []
    
    # Create documents folder if it doesn't exist
    os.makedirs(DOCUMENTS_FOLDER, exist_ok=True)
    
    # Load PDF files
    pdf_files = glob.glob(os.path.join(DOCUMENTS_FOLDER, "*.pdf"))
    for pdf_file in pdf_files:
        print(f"Loading PDF: {pdf_file}")
        loader = PyPDFLoader(pdf_file)
        docs = loader.load()
        # Add source metadata
        for doc in docs:
            doc.metadata['source_file'] = os.path.basename(pdf_file)
            doc.metadata['file_type'] = 'pdf'
        documents.extend(docs)
    
    # Load text files
    txt_files = glob.glob(os.path.join(DOCUMENTS_FOLDER, "*.txt"))
    for txt_file in txt_files:
        print(f"Loading text file: {txt_file}")
        loader = TextLoader(txt_file, encoding='utf-8')
        docs = loader.load()
        # Add source metadata
        for doc in docs:
            doc.metadata['source_file'] = os.path.basename(txt_file)
            doc.metadata['file_type'] = 'txt'
        documents.extend(docs)
    
    if not documents:
        raise FileNotFoundError(f"No PDF or text files found in: {DOCUMENTS_FOLDER}")
    
    print(f"Loaded {len(documents)} documents from {len(pdf_files + txt_files)} files")
    return documents

# Initialize vector store
def initialize_vector_store():
    # Load multiple documents
    docs = load_multiple_documents()
    
    # Split documents
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, 
        chunk_overlap=200,
        separators=["\n\n", "\n", " ", ""]
    )
    final_documents = text_splitter.split_documents(docs)
    
    print(f"Split into {len(final_documents)} chunks")
    
    # Check and manage Atlas Search Index
    response = get_atlas_search_index()
    if response.status_code == 200:
        print("Deleting existing Atlas Search Index...")
        delete_response = delete_atlas_search_index()
        if delete_response.status_code == 204:
            # Wait for index deletion to complete
            print("Waiting for index deletion to complete...")
            while get_atlas_search_index().status_code != 404:
                time.sleep(5)
        else:
            raise Exception(f"Failed to delete existing Atlas Search Index: {delete_response.status_code}, Response: {delete_response.text}")
    elif response.status_code != 404:
        raise Exception(f"Failed to check Atlas Search Index: {response.status_code}, Response: {response.text}")
    
    # Clear existing collection
    db[collection_name].delete_many({})
    
    # Store embeddings with Gemini embeddings
    vector_search = MongoDBAtlasVectorSearch.from_documents(
        documents=final_documents,
        embedding=embeddings,
        collection=db[collection_name],
        index_name=INDEX_NAME,
    )
    
    # Debug: Verify documents in collection
    doc_count = db[collection_name].count_documents({})
    print(f"Number of documents in {collection_name}: {doc_count}")
    if doc_count > 0:
        sample_doc = db[collection_name].find_one()
        print(f"Sample document structure (keys): {list(sample_doc.keys())}")
    
    # Create new Atlas Search Index
    print("Creating new Atlas Search Index...")
    create_response = create_atlas_search_index()
    print(f"Atlas Search Index creation status: {create_response.status_code}")
    
    # Wait for index to be ready
    print("Waiting for index to be ready...")
    time.sleep(30)  # Give some time for index to be created
    
    return vector_search

# Extract lead information using Gemini API
def extract_lead_info(session_id):
    # Get chat history
    chat_doc = chat_collection.find_one({"session_id": session_id})
    if not chat_doc or "messages" not in chat_doc:
        return
    
    # Convert conversation to plain text
    conversation = "\n".join([f"{msg['role']}: {msg['content']}" for msg in chat_doc["messages"]])
    
    try:
        # Use the Gemini LLM to extract lead info
        response = lead_extraction_llm.invoke(LEAD_EXTRACTION_PROMPT.format(conversation=conversation))
        response_text = response.content.strip()
        
        # Extract JSON from potential markdown code blocks
        if "```json" in response_text or "```" in response_text:
            import re
            json_match = re.search(r"```(?:json)?\n(.*?)\n```", response_text, re.DOTALL)
            if json_match:
                response_text = json_match.group(1).strip()
        
        try:
            lead_data = json.loads(response_text)
            print(f"Successfully parsed lead data: {lead_data}")
        except json.JSONDecodeError as e:
            print(f"Failed to parse JSON from Gemini response: {response_text}")
            print(f"JSON error: {str(e)}")
            
            # Alternative approach: Use regex to find JSON-like structure
            import re
            json_pattern = r'\{[^}]*"name"[^}]*"email_id"[^}]*"contact_number"[^}]*"location"[^}]*"service_interest"[^}]*\}'
            json_match = re.search(json_pattern, response_text, re.DOTALL)
            
            if json_match:
                try:
                    lead_data = json.loads(json_match.group(0))
                    print(f"Extracted JSON using regex: {lead_data}")
                except json.JSONDecodeError:
                    # Fallback if all parsing fails
                    lead_data = {
                        "name": "",
                        "email_id": "",
                        "contact_number": "",
                        "location": "",
                        "service_interest": "",
                        "appointment_date": "",
                        "appointment_time": "",
                        "parsing_error": "Failed to parse response"
                    }
            else:
                # Final fallback
                lead_data = {
                    "name": "",
                    "email_id": "",
                    "contact_number": "",
                    "location": "",
                    "service_interest": "",
                    "appointment_date": "",
                    "appointment_time": "",
                    "raw_response": response_text[:500]
                }
        
        # Add session_id & timestamp
        lead_data["session_id"] = session_id
        lead_data["updated_at"] = datetime.utcnow()
        
        # Add LLM metadata for tracking
        lead_data["extraction_model"] = "gemini_" + GEMINI_MODEL
        
        # Save to MongoDB
        lead_collection.update_one(
            {"session_id": session_id},
            {"$set": lead_data},
            upsert=True
        )
    except Exception as e:
        print(f"[Lead Extraction Error] {e}")

# Initialize vector store
try:
    vector_search = initialize_vector_store()
    print("Vector store initialized successfully")
except Exception as e:
    print(f"Failed to initialize vector store: {e}")
    raise

def handle_chat(session_id: str, user_input: str) -> str:
    # Create RAG pipeline with enhanced retrieval
    document_chain = create_stuff_documents_chain(llm, qa_prompt)
    retriever = vector_search.as_retriever(
        search_type="similarity_score_threshold",
        search_kwargs={"k": 8, "score_threshold": 0.6}
    )
    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
    retrieval_chain = create_retrieval_chain(history_aware_retriever, document_chain)

    conversational_rag_chain = RunnableWithMessageHistory(
        retrieval_chain,
        get_session_history,
        input_messages_key="input",
        history_messages_key="chat_history",
        output_messages_key="answer",
    )

    # Invoke RAG chain
    response = conversational_rag_chain.invoke(
        {"input": user_input},
        config={"configurable": {"session_id": session_id}}
    )
    answer = response['answer']

    # Store in MongoDB
    chat_collection.update_one(
        {"session_id": session_id},
        {
            "$push": {
                "messages": {
                    "$each": [
                        {"role": "user", "content": user_input, "timestamp": datetime.utcnow()},
                        {"role": "assistant", "content": answer, "timestamp": datetime.utcnow()}
                    ]
                }
            },
            "$setOnInsert": {"created_at": datetime.utcnow()},
        },
        upsert=True
    )

    # Trigger lead extraction if enough messages
    message_count = len(chat_collection.find_one({"session_id": session_id}).get("messages", []))
    if message_count >= 4:
        extract_lead_info(session_id)

    return answer
# @app.before_first_request
# def list_routes():
#     print("Registered routes:")
#     for rule in app.url_map.iter_rules():
#         print(f"{rule.methods} {rule.rule}")

# Routes
@app.route('/')
def index():
    return render_template('index.html')


@app.route("/check_ffmpeg")
def check_ffmpeg():
    try:
        output = subprocess.check_output(["ffmpeg", "-version"], stderr=subprocess.STDOUT)
        return Response(output, mimetype="text/plain")
    except Exception as e:
        return f"FFmpeg not found or failed to execute: {e}", 500

@app.route('/generate_session', methods=['GET'])
def generate_session():
    session_id = str(uuid.uuid4())
    return jsonify({"session_id": session_id})

@app.route('/chat', methods=['POST'])
def chat():
    data = request.json
    user_input = data.get('message')
    session_id = data.get('session_id', str(uuid.uuid4()))
    
    if not user_input:
        return jsonify({'error': 'No input provided'}), 400
    
    try:
        answer = handle_chat(session_id, user_input)
        return jsonify({'response': answer}), 200
    except Exception as e:
        print(f"Chat error: {str(e)}")
        return jsonify({'error': f'An error occurred: {str(e)}'}), 500
    
#     # Create RAG pipeline with enhanced retrieval
#     document_chain = create_stuff_documents_chain(llm, qa_prompt)
    
#     # Enhanced retriever with better similarity threshold
#     retriever = vector_search.as_retriever(
#         search_type="similarity_score_threshold",
#         search_kwargs={
#             "k": 8,  # Increased to get more relevant context
#             "score_threshold": 0.6  # Adjusted for Gemini embeddings
#         }
#     )
    
#     history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
#     retrieval_chain = create_retrieval_chain(history_aware_retriever, document_chain)
    
#     conversational_rag_chain = RunnableWithMessageHistory(
#         retrieval_chain,
#         get_session_history,
#         input_messages_key="input",
#         history_messages_key="chat_history",
#         output_messages_key="answer",
#     )
    
#     try:
#         # Get response from RAG
#         response = conversational_rag_chain.invoke(
#             {"input": user_input},
#             config={"configurable": {"session_id": session_id}}
#         )
#         answer = response['answer']
        
#         # Log retrieved context for debugging
#         context_docs = response.get('context', [])
#         print(f"Retrieved {len(context_docs)} context documents for query: {user_input}")
        
#         # Store message in MongoDB
#         chat_collection.update_one(
#             {"session_id": session_id},
#             {
#                 "$push": {
#                     "messages": {
#                         "$each": [
#                             {"role": "user", "content": user_input, "timestamp": datetime.utcnow()},
#                             {"role": "assistant", "content": answer, "timestamp": datetime.utcnow()}
#                         ]
#                     }
#                 },
#                 "$setOnInsert": {"created_at": datetime.utcnow()},
#             },
#             upsert=True
#         )
        
#         # Extract lead info after sufficient conversation
#         message_count = len(chat_collection.find_one({"session_id": session_id}).get("messages", []))
#         if message_count >= 4:  # Extract after 2 user messages
#             extract_lead_info(session_id)
        
#         return jsonify({'response': answer}), 200
#     except Exception as e:
#         print(f"Chat error: {str(e)}")
#         return jsonify({'error': f'An error occurred: {str(e)}'}), 500
    
# # --------------- voice chat (NEW) ----------------

# # ==== ElevenLabs Integration – route start ====

# @app.route("/chat_voice", methods=["POST"])
# def chat_voice():
#     if not eleven_client:
#         return jsonify({"error": "Voice service not configured"}), 503

#     # 1️⃣  Validate upload
#     if "audio" not in request.files:
#         return jsonify({"error": "No audio file provided"}), 400
#     # ✅ DEBUG LOGS GO HERE
#     try:
#         print("===== DEBUG: /chat_voice called =====")
#         print("Request headers:", request.headers)
#         print("Request form:", request.form)
#         print("Request files:", request.files)
#         raw_file = request.files["audio"]
#         session_id = request.form.get("session_id", str(uuid.uuid4()))
#         language   = request.form.get("language")  or None       # optional
#         # transcript = stt_transcribe(audio_bytes, language)
#         voice_id   = request.form.get("voice_id", ELEVENLABS_VOICE_ID)

#         # 2️⃣  Convert WEBM → WAV (16-bit 48 kHz)
#         webm_bytes = raw_file.read()
#         audio_seg  = AudioSegment.from_file(BytesIO(webm_bytes), format="webm")
#         wav_io     = BytesIO()
#         audio_seg.export(wav_io, format="wav")
#         wav_bytes  = wav_io.getvalue()
#         print("✅ WEBM converted to WAV – size", len(wav_bytes))

#         # 3️⃣  Speech-to-text
#         user_input = stt_transcribe(wav_bytes, language)
#         print("🗣️  Transcript:", user_input or "(empty)")

#         if not user_input.strip():
#             return jsonify({"error": "Could not detect speech"}), 400

#         # 4️⃣  RAG chat
#         answer = handle_chat(session_id, user_input)
#         print("🤖 Assistant:", answer[:80], "…")

#         # 5️⃣  Text-to-speech
#         tts_bytes = tts_generate(answer, voice_id)
#         audio_b64 = base64.b64encode(tts_bytes).decode()

#         # 6️⃣  Return both text + voice
#         return jsonify({
#             "transcript": user_input,
#             "response":   answer,
#             "audio_b64":  audio_b64,
#             "audio_url": f"data:audio/mp3;base64,{audio_b64}",  # ← ADD THIS LINE
#             "session_id": session_id,
#         }), 200

#     except Exception as e:
#         # print full traceback for easier debugging
#         import traceback, sys
#         traceback.print_exc(file=sys.stdout)
#         return jsonify({"error": f"Voice pipeline failed: {e}"}), 500

@app.route("/chat_voice", methods=["POST"])
def chat_voice():
    print("=" * 60)
    print("🎤 VOICE CHAT DEBUG - Starting request processing")
    print("=" * 60)
    
    try:
        # Step 1: Check ElevenLabs configuration
        print("Step 1: Checking ElevenLabs configuration...")
        if not ELEVENLABS_API_KEY:
            error_msg = "ELEVENLABS_API_KEY environment variable not set"
            print(f"❌ {error_msg}")
            return jsonify({"error": error_msg}), 503
        
        if not eleven_client:
            error_msg = "ElevenLabs client not initialized"
            print(f"❌ {error_msg}")
            return jsonify({"error": error_msg}), 503
        
        print(f"✅ ElevenLabs API Key: {'*' * (len(ELEVENLABS_API_KEY)-8) + ELEVENLABS_API_KEY[-4:]}")
        print(f"✅ ElevenLabs client initialized")
        
        # Step 2: Check request data
        print("\nStep 2: Checking request data...")
        print(f"Request method: {request.method}")
        print(f"Request headers: {dict(request.headers)}")
        print(f"Request form keys: {list(request.form.keys())}")
        print(f"Request files keys: {list(request.files.keys())}")
        
        if "audio" not in request.files:
            error_msg = "No audio file provided in request"
            print(f"❌ {error_msg}")
            return jsonify({"error": error_msg}), 400
            
        raw_file = request.files["audio"]
        if raw_file.filename == '':
            error_msg = "Empty audio filename"
            print(f"❌ {error_msg}")
            return jsonify({"error": error_msg}), 400
        
        print(f"✅ Audio file received: {raw_file.filename}")
        
        # Step 3: Get form parameters
        print("\nStep 3: Getting form parameters...")
        session_id = request.form.get("session_id", str(uuid.uuid4()))
        language = request.form.get("language")
        voice_id = request.form.get("voice_id", ELEVENLABS_VOICE_ID)
        
        print(f"Session ID: {session_id}")
        print(f"Language: {language}")
        print(f"Voice ID: {voice_id}")
        
        # Step 4: Read and validate audio file
        print("\nStep 4: Reading audio file...")
        try:
            webm_bytes = raw_file.read()
            file_size = len(webm_bytes)
            print(f"✅ Audio file size: {file_size} bytes")
            
            if file_size == 0:
                error_msg = "Audio file is empty"
                print(f"❌ {error_msg}")
                return jsonify({"error": error_msg}), 400
                
            if file_size > 50 * 1024 * 1024:  # 50MB limit
                error_msg = f"Audio file too large: {file_size} bytes"
                print(f"❌ {error_msg}")
                return jsonify({"error": error_msg}), 400
                
        except Exception as read_error:
            error_msg = f"Failed to read audio file: {str(read_error)}"
            print(f"❌ {error_msg}")
            return jsonify({"error": error_msg}), 400
        
        # Step 5: Check FFmpeg availability
        print("\nStep 5: Checking FFmpeg...")
        try:
            ffmpeg_output = subprocess.check_output(["ffmpeg", "-version"], stderr=subprocess.STDOUT)
            print("✅ FFmpeg is available")
            print(f"FFmpeg version: {ffmpeg_output.decode()[:100]}...")
        except subprocess.CalledProcessError as e:
            error_msg = f"FFmpeg error: {str(e)}"
            print(f"❌ {error_msg}")
            return jsonify({"error": error_msg}), 500
        except FileNotFoundError:
            error_msg = "FFmpeg not found - please install FFmpeg"
            print(f"❌ {error_msg}")
            return jsonify({"error": error_msg}), 500
        
        # Step 6: Convert audio format
        print("\nStep 6: Converting audio format...")
        try:
            print("Creating AudioSegment from WEBM data...")
            audio_seg = AudioSegment.from_file(BytesIO(webm_bytes), format="webm")
            print(f"✅ Original audio: {len(audio_seg)}ms, {audio_seg.frame_rate}Hz, {audio_seg.channels} channels")
            
            # Convert to standard format
            print("Converting to standard format...")
            audio_seg = audio_seg.set_frame_rate(16000).set_channels(1)
            print(f"✅ Converted audio: {len(audio_seg)}ms, {audio_seg.frame_rate}Hz, {audio_seg.channels} channels")
            
            # Export to WAV
            print("Exporting to WAV...")
            wav_io = BytesIO()
            audio_seg.export(wav_io, format="wav")
            wav_bytes = wav_io.getvalue()
            print(f"✅ WAV export complete: {len(wav_bytes)} bytes")
            
        except Exception as conversion_error:
            error_msg = f"Audio conversion failed: {str(conversion_error)}"
            print(f"❌ {error_msg}")
            import traceback
            print("Conversion error traceback:")
            traceback.print_exc()
            return jsonify({"error": error_msg}), 500
        
        # Step 7: Speech-to-text
        print("\nStep 7: Speech-to-text processing...")
        try:
            print("Calling ElevenLabs STT...")
            user_input = stt_transcribe(wav_bytes, language)
            print(f"✅ STT completed: '{user_input}'")
            
            if not user_input or not user_input.strip():
                error_msg = "No speech detected in audio"
                print(f"❌ {error_msg}")
                return jsonify({"error": error_msg}), 400
                
        except Exception as stt_error:
            error_msg = f"Speech-to-text failed: {str(stt_error)}"
            print(f"❌ {error_msg}")
            import traceback
            print("STT error traceback:")
            traceback.print_exc()
            return jsonify({"error": error_msg}), 500
        
        # Step 8: Chat processing
        print("\nStep 8: Processing chat...")
        try:
            print(f"Calling handle_chat with: '{user_input[:50]}...'")
            answer = handle_chat(session_id, user_input)
            print(f"✅ Chat response: '{answer[:100]}...'")
            
            if not answer:
                answer = "I apologize, but I couldn't generate a response."
                
        except Exception as chat_error:
            error_msg = f"Chat processing failed: {str(chat_error)}"
            print(f"❌ {error_msg}")
            import traceback
            print("Chat error traceback:")
            traceback.print_exc()
            return jsonify({"error": error_msg}), 500
        
        # Step 9: Text-to-speech
        print("\nStep 9: Text-to-speech processing...")
        try:
            print(f"Generating TTS for: '{answer[:50]}...'")
            tts_bytes = tts_generate(answer, voice_id)
            print(f"✅ TTS generated: {len(tts_bytes)} bytes")
            
            if not tts_bytes:
                error_msg = "TTS generated empty audio"
                print(f"❌ {error_msg}")
                return jsonify({"error": error_msg}), 500
                
            # Encode to base64
            audio_b64 = base64.b64encode(tts_bytes).decode()
            print(f"✅ Base64 encoding complete: {len(audio_b64)} characters")
            
        except Exception as tts_error:
            error_msg = f"Text-to-speech failed: {str(tts_error)}"
            print(f"❌ {error_msg}")
            import traceback
            print("TTS error traceback:")
            traceback.print_exc()
            return jsonify({"error": error_msg}), 500
        
        # Step 10: Return response
        print("\nStep 10: Preparing response...")
        response_data = {
            "transcript": user_input,
            "response": answer,
            "audio_b64": audio_b64,
            "audio_url": f"data:audio/mp3;base64,{audio_b64}",
            "session_id": session_id,
        }
        
        print("✅ Voice chat processing completed successfully!")
        print("=" * 60)
        
        return jsonify(response_data), 200
        
    except Exception as e:
        error_msg = f"Unexpected error in voice chat: {str(e)}"
        print(f"❌ CRITICAL ERROR: {error_msg}")
        
        import traceback
        print("Full error traceback:")
        traceback.print_exc()
        print("=" * 60)
        
        return jsonify({"error": error_msg}), 500  
    

@app.route('/leads', methods=['GET'])
def get_leads():
    # Simple admin route to get all leads (should be protected in production)
    leads = list(lead_collection.find({}, {"_id": 0}))
    return jsonify(leads)

@app.route('/upload_documents', methods=['POST'])
def upload_documents():
    """Route to handle document uploads and reinitialize vector store"""
    try:
        if 'files' not in request.files:
            return jsonify({'error': 'No files uploaded'}), 400
        
        files = request.files.getlist('files')
        uploaded_files = []
        
        for file in files:
            if file.filename == '':
                continue
            
            # Check file type
            if not (file.filename.lower().endswith('.pdf') or file.filename.lower().endswith('.txt')):
                continue
            
            # Save file
            filename = file.filename
            file_path = os.path.join(DOCUMENTS_FOLDER, filename)
            file.save(file_path)
            uploaded_files.append(filename)
        
        if uploaded_files:
            # Reinitialize vector store with new documents
            global vector_search
            vector_search = initialize_vector_store()
            
            return jsonify({
                'message': f'Successfully uploaded and processed {len(uploaded_files)} files',
                'files': uploaded_files
            }), 200
        else:
            return jsonify({'error': 'No valid PDF or text files uploaded'}), 400
            
    except Exception as e:
        return jsonify({'error': f'Upload failed: {str(e)}'}), 500
    
# Add this diagnostic route
@app.route('/voice_test', methods=['GET'])
def voice_test():
    """Test voice functionality step by step"""
    results = {}
    
    # Test 1: Environment variables
    results['env_vars'] = {
        'ELEVENLABS_API_KEY': bool(ELEVENLABS_API_KEY),
        'ELEVENLABS_VOICE_ID': ELEVENLABS_VOICE_ID
    }
    
    # Test 2: ElevenLabs client
    results['client'] = bool(eleven_client)
    
    # Test 3: FFmpeg
    try:
        subprocess.check_output(["ffmpeg", "-version"], stderr=subprocess.STDOUT)
        results['ffmpeg'] = True
    except:
        results['ffmpeg'] = False
    
    # Test 4: ElevenLabs API connectivity
    if eleven_client:
        try:
            voices = eleven_client.voices.get_all()
            results['api_connection'] = True
            results['available_voices'] = len(voices.voices) if voices else 0
        except Exception as e:
            results['api_connection'] = False
            results['api_error'] = str(e)
    else:
        results['api_connection'] = False
    
    # Test 5: MongoDB connection
    try:
        client.admin.command('ping')
        results['mongodb'] = True
    except:
        results['mongodb'] = False
    
    return jsonify(results)

@app.route('/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    try:
        # Check database connection
        client.admin.command('ping')
        
        # Check vector store
        doc_count = db[collection_name].count_documents({})
        
        return jsonify({
            'status': 'healthy',
            'database': 'connected',
            'documents_indexed': doc_count,
            'model': GEMINI_MODEL
        }), 200
    except Exception as e:
        return jsonify({
            'status': 'unhealthy',
            'error': str(e)
        }), 500

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8000, debug=True)
    